<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Segment Anything 2 ‚Äì Paper Review by Linlin Zhang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f9f9f9;
      margin: 0;
      padding: 0;
    }
    .container {
      max-width: 860px;
      margin: 2rem auto;
      background: white;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
    }
    h1, h2 {
      color: #1e2d50;
    }
    p {
      line-height: 1.6;
    }
    .section {
      margin-top: 2rem;
    }
    a {
      color: #2c5cc5;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üìÑ Segment Anything 2 (2024)</h1>
    <p><strong>Paper Title:</strong> Segment Anything 2: Segment Anything in Images and Videos<br>
    <strong>Authors:</strong> Nikhila Ravi et al.<br>
    <strong>Review by:</strong> Linlin Zhang<br>
    <strong>Course:</strong> COMS6998E ‚Äì Spring 2025</p>

    <div class="section">
      <h2>üß† Main Contribution</h2>
      <p>
        Segment Anything 2 (SAM 2) extends the original SAM framework from static images to dynamic video content. The paper introduces a unified model for promptable segmentation across both spatial and temporal domains. A major innovation lies in its ability to perform zero-shot segmentation in videos using a learned video foundation model. The authors propose several strategies including spatio-temporal mask propagation, efficient memory management, and cross-frame attention to handle long sequences. The system is trained on a new large-scale video dataset to support strong generalization across tasks.
      </p>
    </div>

    <div class="section">
      <h2>‚ö†Ô∏è Limitations & Failure Cases</h2>
      <p>
        While SAM 2 excels in promptable video segmentation, it still faces challenges in accurately segmenting objects that undergo significant deformation, occlusion, or fast motion. Additionally, the memory and computational requirements for long video sequences are non-trivial, which limits deployment in real-time or resource-constrained settings. The quality of segmentations degrades in low-light conditions and fast camera motion due to weak temporal consistency. The reliance on high-quality masks from SAM 1 also introduces upstream biases.
      </p>
    </div>

    <div class="section">
      <h2>üí° Suggested Improvements</h2>
      <p>
        Future work could explore more robust temporal modeling with lightweight recurrent or transformer architectures designed for long-horizon memory. Incorporating multi-modal cues such as audio or IMU data might help maintain segmentation stability during fast motion. Self-supervised pretraining on video-level transformations could reduce the dependence on manual annotations. Finally, integrating dynamic prompt adaptation across frames (instead of static prompts) could significantly improve long-term consistency in real-world deployment scenarios.
      </p>
    </div>

    <footer style="margin-top: 3rem; font-size: 0.85rem; color: gray;">
      Reviewed by Linlin Zhang | Spring 2025 ‚Äì Columbia University
    </footer>
  </div>
</body>
</html>
