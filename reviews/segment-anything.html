<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Segment Anything ‚Äì Paper Review by Linlin Zhang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f9f9f9;
      margin: 0;
      padding: 0;
    }
    .container {
      max-width: 860px;
      margin: 2rem auto;
      background: white;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
    }
    h1, h2 {
      color: #1e2d50;
    }
    p {
      line-height: 1.6;
    }
    .section {
      margin-top: 2rem;
    }
    a {
      color: #2c5cc5;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üìÑ Segment Anything (2023)</h1>
    <p><strong>Paper Title:</strong> Segment Anything<br>
    <strong>Authors:</strong> Alexander Kirillov et al.<br>
    <strong>Review by:</strong> Linlin Zhang<br>
    <strong>Course:</strong> COMS6998E ‚Äì Spring 2025</p>

    <div class="section">
      <h2>üß† Main Contribution</h2>
      <p>
        This paper introduces the Segment Anything Model (SAM), a foundation model designed for promptable image segmentation. SAM can generate object masks based on various input prompts (points, boxes, or masks) and supports zero-shot generalization. The authors also release the SA-1B dataset, comprising over 1 billion high-quality masks collected from 11 million images using a model-in-the-loop approach.
      </p>
    </div>

    <div class="section">
      <h2>üõ†Ô∏è Architecture</h2>
      <p>
        SAM includes three components: (1) an image encoder (ViT-H) that processes images once per input, (2) a prompt encoder for sparse/dense inputs, and (3) a lightweight mask decoder that outputs multiple candidate masks with confidence scores. This design enables interactive and efficient segmentation in both research and practical settings.
      </p>
    </div>

    <div class="section">
      <h2>‚ö†Ô∏è Limitations & Challenges</h2>
      <p>
        Despite its versatility, SAM faces several challenges: high compute demands, struggles with precise boundaries in low-contrast or ambiguous regions, sensitivity to prompt quality, and dataset-induced biases due to semi-automated labeling. Additionally, it may require user guidance when segmenting objects in complex scenes.
      </p>
    </div>

    <div class="section">
      <h2>üìä Empirical Performance</h2>
      <p>
        SAM demonstrates strong zero-shot segmentation across benchmarks like COCO, LVIS, and ADE20K. Its interactive performance is especially compelling, producing accurate masks with minimal user input. The system significantly lowers the barrier to generating annotated data for downstream vision tasks.
      </p>
    </div>

    <div class="section">
      <h2>üí° Reflections</h2>
      <p>
        SAM marks a shift in segmentation from task-specific models to a generalized, reusable system. It parallels the impact of CLIP for vision-language tasks. SAM and SA-1B provide a powerful foundation for applications in robotics, AR/VR, and embodied AI. The model's promptable nature also hints at future integration with multimodal agents, where segmentation can be guided by language or touch.
      </p>
    </div>

    <footer style="margin-top: 3rem; font-size: 0.85rem; color: gray;">
      Reviewed by Linlin Zhang | Spring 2025 ‚Äì Columbia University
    </footer>
  </div>
</body>
</html>
